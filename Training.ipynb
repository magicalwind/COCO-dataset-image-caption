{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1306/414113 [00:00<01:07, 6143.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.80s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [01:01<00:00, 6757.60it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('./cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 10          # batch size\n",
    "vocab_threshold = 0.4       # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 256           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/41412], Loss: 4.0002, Perplexity: 54.6077\n",
      "Epoch [1/3], Step [200/41412], Loss: 4.0060, Perplexity: 54.92536\n",
      "Epoch [1/3], Step [300/41412], Loss: 3.8952, Perplexity: 49.1673\n",
      "Epoch [1/3], Step [400/41412], Loss: 3.7748, Perplexity: 43.58790\n",
      "Epoch [1/3], Step [500/41412], Loss: 3.4968, Perplexity: 33.0094\n",
      "Epoch [1/3], Step [600/41412], Loss: 3.3878, Perplexity: 29.59977\n",
      "Epoch [1/3], Step [700/41412], Loss: 3.3744, Perplexity: 29.20778\n",
      "Epoch [1/3], Step [800/41412], Loss: 3.3292, Perplexity: 27.9165\n",
      "Epoch [1/3], Step [900/41412], Loss: 3.0967, Perplexity: 22.1246\n",
      "Epoch [1/3], Step [1000/41412], Loss: 3.2101, Perplexity: 24.7805\n",
      "Epoch [1/3], Step [1100/41412], Loss: 3.1116, Perplexity: 22.4580\n",
      "Epoch [1/3], Step [1200/41412], Loss: 3.7805, Perplexity: 43.8390\n",
      "Epoch [1/3], Step [1300/41412], Loss: 2.9828, Perplexity: 19.7428\n",
      "Epoch [1/3], Step [1400/41412], Loss: 3.0570, Perplexity: 21.2630\n",
      "Epoch [1/3], Step [1500/41412], Loss: 2.7453, Perplexity: 15.5687\n",
      "Epoch [1/3], Step [1600/41412], Loss: 3.2470, Perplexity: 25.7141\n",
      "Epoch [1/3], Step [1700/41412], Loss: 3.2569, Perplexity: 25.9699\n",
      "Epoch [1/3], Step [1800/41412], Loss: 3.4798, Perplexity: 32.45185\n",
      "Epoch [1/3], Step [1900/41412], Loss: 3.0996, Perplexity: 22.1899\n",
      "Epoch [1/3], Step [2000/41412], Loss: 2.9872, Perplexity: 19.8308\n",
      "Epoch [1/3], Step [2100/41412], Loss: 3.0092, Perplexity: 20.2713\n",
      "Epoch [1/3], Step [2200/41412], Loss: 2.7558, Perplexity: 15.7329\n",
      "Epoch [1/3], Step [2300/41412], Loss: 2.7633, Perplexity: 15.8521\n",
      "Epoch [1/3], Step [2400/41412], Loss: 3.2022, Perplexity: 24.5869\n",
      "Epoch [1/3], Step [2500/41412], Loss: 2.9376, Perplexity: 18.8710\n",
      "Epoch [1/3], Step [2600/41412], Loss: 2.9642, Perplexity: 19.3801\n",
      "Epoch [1/3], Step [2700/41412], Loss: 2.5434, Perplexity: 12.7232\n",
      "Epoch [1/3], Step [2800/41412], Loss: 3.0904, Perplexity: 21.9869\n",
      "Epoch [1/3], Step [2900/41412], Loss: 2.7888, Perplexity: 16.2611\n",
      "Epoch [1/3], Step [3000/41412], Loss: 2.9080, Perplexity: 18.3202\n",
      "Epoch [1/3], Step [3100/41412], Loss: 2.5511, Perplexity: 12.8214\n",
      "Epoch [1/3], Step [3200/41412], Loss: 2.7604, Perplexity: 15.8063\n",
      "Epoch [1/3], Step [3300/41412], Loss: 2.2694, Perplexity: 9.674059\n",
      "Epoch [1/3], Step [3400/41412], Loss: 2.6604, Perplexity: 14.3027\n",
      "Epoch [1/3], Step [3500/41412], Loss: 2.8320, Perplexity: 16.9792\n",
      "Epoch [1/3], Step [3600/41412], Loss: 2.6840, Perplexity: 14.6434\n",
      "Epoch [1/3], Step [3700/41412], Loss: 2.9269, Perplexity: 18.6690\n",
      "Epoch [1/3], Step [3800/41412], Loss: 3.0532, Perplexity: 21.1827\n",
      "Epoch [1/3], Step [3900/41412], Loss: 2.9885, Perplexity: 19.8559\n",
      "Epoch [1/3], Step [4000/41412], Loss: 2.5454, Perplexity: 12.7479\n",
      "Epoch [1/3], Step [4100/41412], Loss: 2.4121, Perplexity: 11.1574\n",
      "Epoch [1/3], Step [4200/41412], Loss: 2.4158, Perplexity: 11.1992\n",
      "Epoch [1/3], Step [4300/41412], Loss: 2.5166, Perplexity: 12.3869\n",
      "Epoch [1/3], Step [4400/41412], Loss: 3.0950, Perplexity: 22.0869\n",
      "Epoch [1/3], Step [4500/41412], Loss: 2.4430, Perplexity: 11.5073\n",
      "Epoch [1/3], Step [4600/41412], Loss: 2.4386, Perplexity: 11.4574\n",
      "Epoch [1/3], Step [4700/41412], Loss: 3.2960, Perplexity: 27.00314\n",
      "Epoch [1/3], Step [4800/41412], Loss: 2.7878, Perplexity: 16.2450\n",
      "Epoch [1/3], Step [4900/41412], Loss: 2.7311, Perplexity: 15.3500\n",
      "Epoch [1/3], Step [5000/41412], Loss: 2.3888, Perplexity: 10.9008\n",
      "Epoch [1/3], Step [5100/41412], Loss: 2.6147, Perplexity: 13.66281\n",
      "Epoch [1/3], Step [5200/41412], Loss: 2.3267, Perplexity: 10.2444\n",
      "Epoch [1/3], Step [5300/41412], Loss: 2.3381, Perplexity: 10.36186\n",
      "Epoch [1/3], Step [5400/41412], Loss: 2.5341, Perplexity: 12.6047\n",
      "Epoch [1/3], Step [5500/41412], Loss: 2.4584, Perplexity: 11.6865\n",
      "Epoch [1/3], Step [5600/41412], Loss: 2.7068, Perplexity: 14.9811\n",
      "Epoch [1/3], Step [5700/41412], Loss: 2.5934, Perplexity: 13.3750\n",
      "Epoch [1/3], Step [5800/41412], Loss: 2.0132, Perplexity: 7.487641\n",
      "Epoch [1/3], Step [5900/41412], Loss: 2.5380, Perplexity: 12.65470\n",
      "Epoch [1/3], Step [6000/41412], Loss: 2.7770, Perplexity: 16.0715\n",
      "Epoch [1/3], Step [6100/41412], Loss: 2.4750, Perplexity: 11.8814\n",
      "Epoch [1/3], Step [6200/41412], Loss: 2.7679, Perplexity: 15.9253\n",
      "Epoch [1/3], Step [6300/41412], Loss: 3.0465, Perplexity: 21.0411\n",
      "Epoch [1/3], Step [6400/41412], Loss: 2.7386, Perplexity: 15.4658\n",
      "Epoch [1/3], Step [6500/41412], Loss: 2.8959, Perplexity: 18.0989\n",
      "Epoch [1/3], Step [6600/41412], Loss: 2.4230, Perplexity: 11.2796\n",
      "Epoch [1/3], Step [6700/41412], Loss: 2.5766, Perplexity: 13.1521\n",
      "Epoch [1/3], Step [6800/41412], Loss: 2.6119, Perplexity: 13.6252\n",
      "Epoch [1/3], Step [6900/41412], Loss: 2.5677, Perplexity: 13.0358\n",
      "Epoch [1/3], Step [7000/41412], Loss: 2.7864, Perplexity: 16.2223\n",
      "Epoch [1/3], Step [7100/41412], Loss: 2.3608, Perplexity: 10.5994\n",
      "Epoch [1/3], Step [7200/41412], Loss: 2.1282, Perplexity: 8.39975\n",
      "Epoch [1/3], Step [7300/41412], Loss: 2.1416, Perplexity: 8.51329\n",
      "Epoch [1/3], Step [7400/41412], Loss: 3.0948, Perplexity: 22.0825\n",
      "Epoch [1/3], Step [7500/41412], Loss: 2.8364, Perplexity: 17.05504\n",
      "Epoch [1/3], Step [7600/41412], Loss: 2.6841, Perplexity: 14.6446\n",
      "Epoch [1/3], Step [7700/41412], Loss: 2.6148, Perplexity: 13.6647\n",
      "Epoch [1/3], Step [7800/41412], Loss: 2.7196, Perplexity: 15.1742\n",
      "Epoch [1/3], Step [7900/41412], Loss: 2.4544, Perplexity: 11.6399\n",
      "Epoch [1/3], Step [8000/41412], Loss: 2.2624, Perplexity: 9.60618\n",
      "Epoch [1/3], Step [8100/41412], Loss: 2.4691, Perplexity: 11.8119\n",
      "Epoch [1/3], Step [8200/41412], Loss: 2.8544, Perplexity: 17.3644\n",
      "Epoch [1/3], Step [8300/41412], Loss: 2.6831, Perplexity: 14.6309\n",
      "Epoch [1/3], Step [8400/41412], Loss: 2.5932, Perplexity: 13.3718\n",
      "Epoch [1/3], Step [8500/41412], Loss: 2.3564, Perplexity: 10.5531\n",
      "Epoch [1/3], Step [8600/41412], Loss: 2.5439, Perplexity: 12.7288\n",
      "Epoch [1/3], Step [8700/41412], Loss: 3.1412, Perplexity: 23.1321\n",
      "Epoch [1/3], Step [8800/41412], Loss: 2.4823, Perplexity: 11.9684\n",
      "Epoch [1/3], Step [8900/41412], Loss: 2.8802, Perplexity: 17.8185\n",
      "Epoch [1/3], Step [9000/41412], Loss: 2.3415, Perplexity: 10.3968\n",
      "Epoch [1/3], Step [9100/41412], Loss: 2.8668, Perplexity: 17.5812\n",
      "Epoch [1/3], Step [9200/41412], Loss: 2.3364, Perplexity: 10.3437\n",
      "Epoch [1/3], Step [9300/41412], Loss: 2.6133, Perplexity: 13.6436\n",
      "Epoch [1/3], Step [9400/41412], Loss: 2.6613, Perplexity: 14.3149\n",
      "Epoch [1/3], Step [9500/41412], Loss: 2.5011, Perplexity: 12.1963\n",
      "Epoch [1/3], Step [9600/41412], Loss: 2.0953, Perplexity: 8.127748\n",
      "Epoch [1/3], Step [9700/41412], Loss: 2.0330, Perplexity: 7.63683\n",
      "Epoch [1/3], Step [9800/41412], Loss: 2.7160, Perplexity: 15.1200\n",
      "Epoch [1/3], Step [9900/41412], Loss: 2.1813, Perplexity: 8.85792\n",
      "Epoch [1/3], Step [10000/41412], Loss: 2.5580, Perplexity: 12.9101\n",
      "Epoch [1/3], Step [10100/41412], Loss: 2.6510, Perplexity: 14.1678\n",
      "Epoch [1/3], Step [10200/41412], Loss: 2.7674, Perplexity: 15.9176\n",
      "Epoch [1/3], Step [10300/41412], Loss: 2.6123, Perplexity: 13.6301\n",
      "Epoch [1/3], Step [10400/41412], Loss: 2.3414, Perplexity: 10.3956\n",
      "Epoch [1/3], Step [10500/41412], Loss: 1.8296, Perplexity: 6.23169\n",
      "Epoch [1/3], Step [10600/41412], Loss: 3.2767, Perplexity: 26.4883\n",
      "Epoch [1/3], Step [10700/41412], Loss: 2.2737, Perplexity: 9.71539\n",
      "Epoch [1/3], Step [10800/41412], Loss: 2.5671, Perplexity: 13.0280\n",
      "Epoch [1/3], Step [10900/41412], Loss: 2.3309, Perplexity: 10.2876\n",
      "Epoch [1/3], Step [11000/41412], Loss: 2.2301, Perplexity: 9.30127\n",
      "Epoch [1/3], Step [11100/41412], Loss: 2.4226, Perplexity: 11.2757\n",
      "Epoch [1/3], Step [11200/41412], Loss: 2.4518, Perplexity: 11.6089\n",
      "Epoch [1/3], Step [11300/41412], Loss: 2.0427, Perplexity: 7.71152\n",
      "Epoch [1/3], Step [11400/41412], Loss: 2.7046, Perplexity: 14.9477\n",
      "Epoch [1/3], Step [11500/41412], Loss: 2.2443, Perplexity: 9.43424\n",
      "Epoch [1/3], Step [11600/41412], Loss: 3.0237, Perplexity: 20.5679\n",
      "Epoch [1/3], Step [11700/41412], Loss: 2.5927, Perplexity: 13.3662\n",
      "Epoch [1/3], Step [11800/41412], Loss: 2.4400, Perplexity: 11.4727\n",
      "Epoch [1/3], Step [11900/41412], Loss: 2.8710, Perplexity: 17.6540\n",
      "Epoch [1/3], Step [12000/41412], Loss: 2.5825, Perplexity: 13.2302\n",
      "Epoch [1/3], Step [12100/41412], Loss: 2.9933, Perplexity: 19.9520\n",
      "Epoch [1/3], Step [12200/41412], Loss: 2.2864, Perplexity: 9.83936\n",
      "Epoch [1/3], Step [12300/41412], Loss: 2.4109, Perplexity: 11.1437\n",
      "Epoch [1/3], Step [12400/41412], Loss: 2.7878, Perplexity: 16.2451\n",
      "Epoch [1/3], Step [12500/41412], Loss: 2.2492, Perplexity: 9.48045\n",
      "Epoch [1/3], Step [12600/41412], Loss: 2.8737, Perplexity: 17.70325\n",
      "Epoch [1/3], Step [12700/41412], Loss: 2.3936, Perplexity: 10.9532\n",
      "Epoch [1/3], Step [12800/41412], Loss: 2.2887, Perplexity: 9.86194\n",
      "Epoch [1/3], Step [12900/41412], Loss: 2.8924, Perplexity: 18.0357\n",
      "Epoch [1/3], Step [13000/41412], Loss: 2.2971, Perplexity: 9.94533\n",
      "Epoch [1/3], Step [13100/41412], Loss: 2.4420, Perplexity: 11.4955\n",
      "Epoch [1/3], Step [13200/41412], Loss: 2.6886, Perplexity: 14.7115\n",
      "Epoch [1/3], Step [13300/41412], Loss: 2.1075, Perplexity: 8.22795\n",
      "Epoch [1/3], Step [13400/41412], Loss: 2.7250, Perplexity: 15.2569\n",
      "Epoch [1/3], Step [13500/41412], Loss: 2.7132, Perplexity: 15.0776\n",
      "Epoch [1/3], Step [13600/41412], Loss: 2.1603, Perplexity: 8.67396\n",
      "Epoch [1/3], Step [13700/41412], Loss: 2.4272, Perplexity: 11.3276\n",
      "Epoch [1/3], Step [13800/41412], Loss: 2.8488, Perplexity: 17.2664\n",
      "Epoch [1/3], Step [13900/41412], Loss: 3.0405, Perplexity: 20.9153\n",
      "Epoch [1/3], Step [14000/41412], Loss: 2.7002, Perplexity: 14.8823\n",
      "Epoch [1/3], Step [14100/41412], Loss: 2.5446, Perplexity: 12.7385\n",
      "Epoch [1/3], Step [14200/41412], Loss: 2.1704, Perplexity: 8.76143\n",
      "Epoch [1/3], Step [14300/41412], Loss: 2.4741, Perplexity: 11.8715\n",
      "Epoch [1/3], Step [14400/41412], Loss: 2.7261, Perplexity: 15.2725\n",
      "Epoch [1/3], Step [14500/41412], Loss: 2.6699, Perplexity: 14.4392\n",
      "Epoch [1/3], Step [14600/41412], Loss: 1.7244, Perplexity: 5.60930\n",
      "Epoch [1/3], Step [14700/41412], Loss: 2.4938, Perplexity: 12.1076\n",
      "Epoch [1/3], Step [14800/41412], Loss: 2.5728, Perplexity: 13.1018\n",
      "Epoch [1/3], Step [14900/41412], Loss: 2.4043, Perplexity: 11.0712\n",
      "Epoch [1/3], Step [15000/41412], Loss: 2.3061, Perplexity: 10.0355\n",
      "Epoch [1/3], Step [15100/41412], Loss: 2.3490, Perplexity: 10.47537\n",
      "Epoch [1/3], Step [15200/41412], Loss: 2.4112, Perplexity: 11.1472\n",
      "Epoch [1/3], Step [15300/41412], Loss: 3.1950, Perplexity: 24.4093\n",
      "Epoch [1/3], Step [15400/41412], Loss: 1.9541, Perplexity: 7.05763\n",
      "Epoch [1/3], Step [15500/41412], Loss: 2.6779, Perplexity: 14.5549\n",
      "Epoch [1/3], Step [15600/41412], Loss: 2.7725, Perplexity: 15.9992\n",
      "Epoch [1/3], Step [15700/41412], Loss: 2.7412, Perplexity: 15.5060\n",
      "Epoch [1/3], Step [15800/41412], Loss: 1.8052, Perplexity: 6.08101\n",
      "Epoch [1/3], Step [15900/41412], Loss: 2.4551, Perplexity: 11.6472\n",
      "Epoch [1/3], Step [16000/41412], Loss: 2.4275, Perplexity: 11.3311\n",
      "Epoch [1/3], Step [16100/41412], Loss: 2.7298, Perplexity: 15.3297\n",
      "Epoch [1/3], Step [16200/41412], Loss: 2.8990, Perplexity: 18.1557\n",
      "Epoch [1/3], Step [16300/41412], Loss: 2.2129, Perplexity: 9.14181\n",
      "Epoch [1/3], Step [16400/41412], Loss: 2.4736, Perplexity: 11.8655\n",
      "Epoch [1/3], Step [16500/41412], Loss: 2.3248, Perplexity: 10.2250\n",
      "Epoch [1/3], Step [16600/41412], Loss: 2.4968, Perplexity: 12.1437\n",
      "Epoch [1/3], Step [16700/41412], Loss: 2.6635, Perplexity: 14.3470\n",
      "Epoch [1/3], Step [16800/41412], Loss: 3.0345, Perplexity: 20.7915\n",
      "Epoch [1/3], Step [16900/41412], Loss: 2.7438, Perplexity: 15.5461\n",
      "Epoch [1/3], Step [17000/41412], Loss: 2.0729, Perplexity: 7.94795\n",
      "Epoch [1/3], Step [17100/41412], Loss: 2.3411, Perplexity: 10.3922\n",
      "Epoch [1/3], Step [17200/41412], Loss: 2.3570, Perplexity: 10.5596\n",
      "Epoch [1/3], Step [17300/41412], Loss: 2.5445, Perplexity: 12.7370\n",
      "Epoch [1/3], Step [17400/41412], Loss: 2.2353, Perplexity: 9.34893\n",
      "Epoch [1/3], Step [17500/41412], Loss: 2.4604, Perplexity: 11.7100\n",
      "Epoch [1/3], Step [17600/41412], Loss: 1.7191, Perplexity: 5.57959\n",
      "Epoch [1/3], Step [17700/41412], Loss: 2.6543, Perplexity: 14.2150\n",
      "Epoch [1/3], Step [17800/41412], Loss: 3.9008, Perplexity: 49.4439\n",
      "Epoch [1/3], Step [17900/41412], Loss: 2.6822, Perplexity: 14.6169\n",
      "Epoch [1/3], Step [18000/41412], Loss: 2.4508, Perplexity: 11.5976\n",
      "Epoch [1/3], Step [18100/41412], Loss: 2.5198, Perplexity: 12.4256\n",
      "Epoch [1/3], Step [18200/41412], Loss: 2.0837, Perplexity: 8.03440\n",
      "Epoch [1/3], Step [18300/41412], Loss: 2.7863, Perplexity: 16.2204\n",
      "Epoch [1/3], Step [18400/41412], Loss: 2.3859, Perplexity: 10.8686\n",
      "Epoch [1/3], Step [18500/41412], Loss: 2.0038, Perplexity: 7.41707\n",
      "Epoch [1/3], Step [18600/41412], Loss: 2.4517, Perplexity: 11.6081\n",
      "Epoch [1/3], Step [18700/41412], Loss: 2.0036, Perplexity: 7.41594\n",
      "Epoch [1/3], Step [18800/41412], Loss: 2.4443, Perplexity: 11.5226\n",
      "Epoch [1/3], Step [18900/41412], Loss: 3.2841, Perplexity: 26.6845\n",
      "Epoch [1/3], Step [19000/41412], Loss: 2.3735, Perplexity: 10.7351\n",
      "Epoch [1/3], Step [19100/41412], Loss: 2.0638, Perplexity: 7.87563\n",
      "Epoch [1/3], Step [19200/41412], Loss: 1.9587, Perplexity: 7.09007\n",
      "Epoch [1/3], Step [19300/41412], Loss: 2.4591, Perplexity: 11.6940\n",
      "Epoch [1/3], Step [19400/41412], Loss: 2.4559, Perplexity: 11.6564\n",
      "Epoch [1/3], Step [19500/41412], Loss: 2.5150, Perplexity: 12.3669\n",
      "Epoch [1/3], Step [19600/41412], Loss: 2.7958, Perplexity: 16.3754\n",
      "Epoch [1/3], Step [19700/41412], Loss: 2.1015, Perplexity: 8.17820\n",
      "Epoch [1/3], Step [19800/41412], Loss: 2.5242, Perplexity: 12.4814\n",
      "Epoch [1/3], Step [19900/41412], Loss: 2.7005, Perplexity: 14.8867\n",
      "Epoch [1/3], Step [20000/41412], Loss: 1.9780, Perplexity: 7.228431\n",
      "Epoch [1/3], Step [20100/41412], Loss: 2.3346, Perplexity: 10.3258\n",
      "Epoch [1/3], Step [20200/41412], Loss: 2.1537, Perplexity: 8.61660\n",
      "Epoch [1/3], Step [20300/41412], Loss: 2.0705, Perplexity: 7.92865\n",
      "Epoch [1/3], Step [20400/41412], Loss: 2.9608, Perplexity: 19.3143\n",
      "Epoch [1/3], Step [20500/41412], Loss: 2.0640, Perplexity: 7.87710\n",
      "Epoch [1/3], Step [20600/41412], Loss: 2.6654, Perplexity: 14.3737\n",
      "Epoch [1/3], Step [20700/41412], Loss: 2.7867, Perplexity: 16.2269\n",
      "Epoch [1/3], Step [20800/41412], Loss: 2.2035, Perplexity: 9.05676\n",
      "Epoch [1/3], Step [20900/41412], Loss: 2.4158, Perplexity: 11.1984\n",
      "Epoch [1/3], Step [21000/41412], Loss: 2.3088, Perplexity: 10.0624\n",
      "Epoch [1/3], Step [21100/41412], Loss: 2.4834, Perplexity: 11.9824\n",
      "Epoch [1/3], Step [21200/41412], Loss: 2.4687, Perplexity: 11.8069\n",
      "Epoch [1/3], Step [21300/41412], Loss: 2.3972, Perplexity: 10.9926\n",
      "Epoch [1/3], Step [21400/41412], Loss: 2.5808, Perplexity: 13.2079\n",
      "Epoch [1/3], Step [21500/41412], Loss: 2.6069, Perplexity: 13.5573\n",
      "Epoch [1/3], Step [21600/41412], Loss: 2.0276, Perplexity: 7.59573\n",
      "Epoch [1/3], Step [21700/41412], Loss: 2.3544, Perplexity: 10.5320\n",
      "Epoch [1/3], Step [21800/41412], Loss: 2.5420, Perplexity: 12.7048\n",
      "Epoch [1/3], Step [21900/41412], Loss: 3.0733, Perplexity: 21.6140\n",
      "Epoch [1/3], Step [22000/41412], Loss: 2.1707, Perplexity: 8.76400\n",
      "Epoch [1/3], Step [22100/41412], Loss: 2.7425, Perplexity: 15.5251\n",
      "Epoch [1/3], Step [22200/41412], Loss: 2.4512, Perplexity: 11.6022\n",
      "Epoch [1/3], Step [22300/41412], Loss: 2.3365, Perplexity: 10.3450\n",
      "Epoch [1/3], Step [22400/41412], Loss: 3.0126, Perplexity: 20.3395\n",
      "Epoch [1/3], Step [22500/41412], Loss: 2.6275, Perplexity: 13.8391\n",
      "Epoch [1/3], Step [22600/41412], Loss: 2.6837, Perplexity: 14.6395\n",
      "Epoch [1/3], Step [22700/41412], Loss: 2.4833, Perplexity: 11.9810\n",
      "Epoch [1/3], Step [22800/41412], Loss: 2.1035, Perplexity: 8.19481\n",
      "Epoch [1/3], Step [22900/41412], Loss: 2.5487, Perplexity: 12.7901\n",
      "Epoch [1/3], Step [23000/41412], Loss: 2.4209, Perplexity: 11.2557\n",
      "Epoch [1/3], Step [23100/41412], Loss: 1.6500, Perplexity: 5.20691\n",
      "Epoch [1/3], Step [23200/41412], Loss: 2.1677, Perplexity: 8.73809\n",
      "Epoch [1/3], Step [23300/41412], Loss: 2.2263, Perplexity: 9.26595\n",
      "Epoch [1/3], Step [23400/41412], Loss: 1.9763, Perplexity: 7.21631\n",
      "Epoch [1/3], Step [23500/41412], Loss: 2.4274, Perplexity: 11.3299\n",
      "Epoch [1/3], Step [23600/41412], Loss: 2.3520, Perplexity: 10.5062\n",
      "Epoch [1/3], Step [23700/41412], Loss: 2.0460, Perplexity: 7.73679\n",
      "Epoch [1/3], Step [23800/41412], Loss: 2.4934, Perplexity: 12.1019\n",
      "Epoch [1/3], Step [23900/41412], Loss: 2.1815, Perplexity: 8.85935\n",
      "Epoch [1/3], Step [24000/41412], Loss: 2.5639, Perplexity: 12.9859\n",
      "Epoch [1/3], Step [24100/41412], Loss: 2.7490, Perplexity: 15.6275\n",
      "Epoch [1/3], Step [24200/41412], Loss: 2.8492, Perplexity: 17.2736\n",
      "Epoch [1/3], Step [24300/41412], Loss: 2.3316, Perplexity: 10.2942\n",
      "Epoch [1/3], Step [24400/41412], Loss: 2.2493, Perplexity: 9.48079\n",
      "Epoch [1/3], Step [24500/41412], Loss: 2.5361, Perplexity: 12.6305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [24600/41412], Loss: 2.8468, Perplexity: 17.2331\n",
      "Epoch [1/3], Step [24700/41412], Loss: 2.1604, Perplexity: 8.67463\n",
      "Epoch [1/3], Step [24800/41412], Loss: 2.3503, Perplexity: 10.4885\n",
      "Epoch [1/3], Step [24900/41412], Loss: 2.6070, Perplexity: 13.5580\n",
      "Epoch [1/3], Step [25000/41412], Loss: 2.4647, Perplexity: 11.7599\n",
      "Epoch [1/3], Step [25100/41412], Loss: 1.7195, Perplexity: 5.58186\n",
      "Epoch [1/3], Step [25200/41412], Loss: 3.0046, Perplexity: 20.1775\n",
      "Epoch [1/3], Step [25300/41412], Loss: 2.8998, Perplexity: 18.1701\n",
      "Epoch [1/3], Step [25400/41412], Loss: 1.7098, Perplexity: 5.528078\n",
      "Epoch [1/3], Step [25500/41412], Loss: 2.9788, Perplexity: 19.6639\n",
      "Epoch [1/3], Step [25600/41412], Loss: 1.8994, Perplexity: 6.68169\n",
      "Epoch [1/3], Step [25700/41412], Loss: 1.8971, Perplexity: 6.66664\n",
      "Epoch [1/3], Step [25800/41412], Loss: 2.6620, Perplexity: 14.3254\n",
      "Epoch [1/3], Step [25900/41412], Loss: 2.5661, Perplexity: 13.0148\n",
      "Epoch [1/3], Step [26000/41412], Loss: 2.4726, Perplexity: 11.8534\n",
      "Epoch [1/3], Step [26100/41412], Loss: 2.5785, Perplexity: 13.1777\n",
      "Epoch [1/3], Step [26200/41412], Loss: 1.7765, Perplexity: 5.90896\n",
      "Epoch [1/3], Step [26300/41412], Loss: 1.8852, Perplexity: 6.58765\n",
      "Epoch [1/3], Step [26400/41412], Loss: 2.6604, Perplexity: 14.3024\n",
      "Epoch [1/3], Step [26500/41412], Loss: 2.4280, Perplexity: 11.3367\n",
      "Epoch [1/3], Step [26600/41412], Loss: 2.1261, Perplexity: 8.382369\n",
      "Epoch [1/3], Step [26700/41412], Loss: 2.0403, Perplexity: 7.69323\n",
      "Epoch [1/3], Step [26800/41412], Loss: 2.9088, Perplexity: 18.3354\n",
      "Epoch [1/3], Step [26900/41412], Loss: 2.3617, Perplexity: 10.6092\n",
      "Epoch [1/3], Step [27000/41412], Loss: 2.6415, Perplexity: 14.0345\n",
      "Epoch [1/3], Step [27100/41412], Loss: 1.8709, Perplexity: 6.49390\n",
      "Epoch [1/3], Step [27200/41412], Loss: 3.1490, Perplexity: 23.3133\n",
      "Epoch [1/3], Step [27300/41412], Loss: 2.6261, Perplexity: 13.8196\n",
      "Epoch [1/3], Step [27400/41412], Loss: 2.2287, Perplexity: 9.28757\n",
      "Epoch [1/3], Step [27500/41412], Loss: 1.9347, Perplexity: 6.92208\n",
      "Epoch [1/3], Step [27600/41412], Loss: 2.4686, Perplexity: 11.8061\n",
      "Epoch [1/3], Step [27700/41412], Loss: 2.1869, Perplexity: 8.90738\n",
      "Epoch [1/3], Step [27800/41412], Loss: 2.0473, Perplexity: 7.74724\n",
      "Epoch [1/3], Step [27900/41412], Loss: 2.4438, Perplexity: 11.5165\n",
      "Epoch [1/3], Step [28000/41412], Loss: 2.5038, Perplexity: 12.2293\n",
      "Epoch [1/3], Step [28100/41412], Loss: 2.5131, Perplexity: 12.3437\n",
      "Epoch [1/3], Step [28200/41412], Loss: 1.8467, Perplexity: 6.33876\n",
      "Epoch [1/3], Step [28300/41412], Loss: 2.8716, Perplexity: 17.6661\n",
      "Epoch [1/3], Step [28400/41412], Loss: 2.3467, Perplexity: 10.4513\n",
      "Epoch [1/3], Step [28500/41412], Loss: 2.2890, Perplexity: 9.86543\n",
      "Epoch [1/3], Step [28600/41412], Loss: 2.4601, Perplexity: 11.7059\n",
      "Epoch [1/3], Step [28700/41412], Loss: 2.5078, Perplexity: 12.2778\n",
      "Epoch [1/3], Step [28800/41412], Loss: 2.6823, Perplexity: 14.6182\n",
      "Epoch [1/3], Step [28900/41412], Loss: 1.9528, Perplexity: 7.04873\n",
      "Epoch [1/3], Step [29000/41412], Loss: 2.7887, Perplexity: 16.2603\n",
      "Epoch [1/3], Step [29100/41412], Loss: 2.4098, Perplexity: 11.1314\n",
      "Epoch [1/3], Step [29200/41412], Loss: 2.4585, Perplexity: 11.6871\n",
      "Epoch [1/3], Step [29300/41412], Loss: 1.8855, Perplexity: 6.58931\n",
      "Epoch [1/3], Step [29400/41412], Loss: 2.1119, Perplexity: 8.26409\n",
      "Epoch [1/3], Step [29500/41412], Loss: 2.2874, Perplexity: 9.84884\n",
      "Epoch [1/3], Step [29600/41412], Loss: 2.5188, Perplexity: 12.4138\n",
      "Epoch [1/3], Step [29700/41412], Loss: 2.1881, Perplexity: 8.91816\n",
      "Epoch [1/3], Step [29800/41412], Loss: 2.0157, Perplexity: 7.50577\n",
      "Epoch [1/3], Step [29900/41412], Loss: 2.7878, Perplexity: 16.2450\n",
      "Epoch [1/3], Step [30000/41412], Loss: 1.8562, Perplexity: 6.39957\n",
      "Epoch [1/3], Step [30100/41412], Loss: 2.0919, Perplexity: 8.10069\n",
      "Epoch [1/3], Step [30200/41412], Loss: 2.3520, Perplexity: 10.5066\n",
      "Epoch [1/3], Step [30300/41412], Loss: 2.5594, Perplexity: 12.9278\n",
      "Epoch [1/3], Step [30400/41412], Loss: 2.1746, Perplexity: 8.79896\n",
      "Epoch [1/3], Step [30500/41412], Loss: 2.0306, Perplexity: 7.61878\n",
      "Epoch [1/3], Step [30600/41412], Loss: 2.6655, Perplexity: 14.3748\n",
      "Epoch [1/3], Step [30700/41412], Loss: 2.3657, Perplexity: 10.6514\n",
      "Epoch [1/3], Step [30800/41412], Loss: 2.0131, Perplexity: 7.48659\n",
      "Epoch [1/3], Step [30900/41412], Loss: 2.5742, Perplexity: 13.1203\n",
      "Epoch [1/3], Step [31000/41412], Loss: 2.5739, Perplexity: 13.1164\n",
      "Epoch [1/3], Step [31100/41412], Loss: 2.2987, Perplexity: 9.96126\n",
      "Epoch [1/3], Step [31200/41412], Loss: 1.8212, Perplexity: 6.17927\n",
      "Epoch [1/3], Step [31300/41412], Loss: 2.6027, Perplexity: 13.4999\n",
      "Epoch [1/3], Step [31400/41412], Loss: 2.1372, Perplexity: 8.47605\n",
      "Epoch [1/3], Step [31500/41412], Loss: 2.4318, Perplexity: 11.3798\n",
      "Epoch [1/3], Step [31600/41412], Loss: 2.1615, Perplexity: 8.68388\n",
      "Epoch [1/3], Step [31700/41412], Loss: 2.1588, Perplexity: 8.66057\n",
      "Epoch [1/3], Step [31800/41412], Loss: 2.0453, Perplexity: 7.73127\n",
      "Epoch [1/3], Step [31900/41412], Loss: 1.9144, Perplexity: 6.78264\n",
      "Epoch [1/3], Step [32000/41412], Loss: 2.0582, Perplexity: 7.83184\n",
      "Epoch [1/3], Step [32100/41412], Loss: 2.6606, Perplexity: 14.3054\n",
      "Epoch [1/3], Step [32200/41412], Loss: 3.2300, Perplexity: 25.2793\n",
      "Epoch [1/3], Step [32300/41412], Loss: 2.6416, Perplexity: 14.0361\n",
      "Epoch [1/3], Step [32400/41412], Loss: 2.9622, Perplexity: 19.3411\n",
      "Epoch [1/3], Step [32500/41412], Loss: 2.2570, Perplexity: 9.55488\n",
      "Epoch [1/3], Step [32600/41412], Loss: 2.3280, Perplexity: 10.2579\n",
      "Epoch [1/3], Step [32700/41412], Loss: 2.8443, Perplexity: 17.1900\n",
      "Epoch [1/3], Step [32800/41412], Loss: 2.3249, Perplexity: 10.2255\n",
      "Epoch [1/3], Step [32900/41412], Loss: 2.3842, Perplexity: 10.8501\n",
      "Epoch [1/3], Step [33000/41412], Loss: 2.4025, Perplexity: 11.0506\n",
      "Epoch [1/3], Step [33100/41412], Loss: 2.0029, Perplexity: 7.41032\n",
      "Epoch [1/3], Step [33200/41412], Loss: 2.5913, Perplexity: 13.3471\n",
      "Epoch [1/3], Step [33300/41412], Loss: 1.7649, Perplexity: 5.84118\n",
      "Epoch [1/3], Step [33400/41412], Loss: 2.4241, Perplexity: 11.2921\n",
      "Epoch [1/3], Step [33500/41412], Loss: 2.2999, Perplexity: 9.97355\n",
      "Epoch [1/3], Step [33600/41412], Loss: 2.1690, Perplexity: 8.74917\n",
      "Epoch [1/3], Step [33700/41412], Loss: 2.0219, Perplexity: 7.55239\n",
      "Epoch [1/3], Step [33800/41412], Loss: 3.1400, Perplexity: 23.1045\n",
      "Epoch [1/3], Step [33900/41412], Loss: 2.1544, Perplexity: 8.62300\n",
      "Epoch [1/3], Step [34000/41412], Loss: 2.5897, Perplexity: 13.3257\n",
      "Epoch [1/3], Step [34100/41412], Loss: 2.0094, Perplexity: 7.45889\n",
      "Epoch [1/3], Step [34200/41412], Loss: 2.5702, Perplexity: 13.0682\n",
      "Epoch [1/3], Step [34300/41412], Loss: 2.1017, Perplexity: 8.17995\n",
      "Epoch [1/3], Step [34400/41412], Loss: 2.4636, Perplexity: 11.7473\n",
      "Epoch [1/3], Step [34500/41412], Loss: 2.4656, Perplexity: 11.7703\n",
      "Epoch [1/3], Step [34600/41412], Loss: 3.2031, Perplexity: 24.6089\n",
      "Epoch [1/3], Step [34700/41412], Loss: 2.5397, Perplexity: 12.6764\n",
      "Epoch [1/3], Step [34800/41412], Loss: 2.1913, Perplexity: 8.94726\n",
      "Epoch [1/3], Step [34900/41412], Loss: 2.2293, Perplexity: 9.29316\n",
      "Epoch [1/3], Step [35000/41412], Loss: 2.1648, Perplexity: 8.71326\n",
      "Epoch [1/3], Step [35100/41412], Loss: 2.2520, Perplexity: 9.50646\n",
      "Epoch [1/3], Step [35200/41412], Loss: 2.5593, Perplexity: 12.9271\n",
      "Epoch [1/3], Step [35300/41412], Loss: 2.2657, Perplexity: 9.63785\n",
      "Epoch [1/3], Step [35400/41412], Loss: 2.4495, Perplexity: 11.5822\n",
      "Epoch [1/3], Step [35500/41412], Loss: 2.4717, Perplexity: 11.8423\n",
      "Epoch [1/3], Step [35600/41412], Loss: 2.1914, Perplexity: 8.94810\n",
      "Epoch [1/3], Step [35700/41412], Loss: 2.1941, Perplexity: 8.97195\n",
      "Epoch [1/3], Step [35800/41412], Loss: 2.5567, Perplexity: 12.8930\n",
      "Epoch [1/3], Step [35900/41412], Loss: 2.3314, Perplexity: 10.2926\n",
      "Epoch [1/3], Step [36000/41412], Loss: 2.0175, Perplexity: 7.51969\n",
      "Epoch [1/3], Step [36100/41412], Loss: 2.6423, Perplexity: 14.0459\n",
      "Epoch [1/3], Step [36200/41412], Loss: 3.5770, Perplexity: 35.7658\n",
      "Epoch [1/3], Step [36300/41412], Loss: 1.8607, Perplexity: 6.428346\n",
      "Epoch [1/3], Step [36400/41412], Loss: 2.2133, Perplexity: 9.146091\n",
      "Epoch [1/3], Step [36500/41412], Loss: 2.7775, Perplexity: 16.0789\n",
      "Epoch [1/3], Step [36600/41412], Loss: 2.4535, Perplexity: 11.6293\n",
      "Epoch [1/3], Step [36700/41412], Loss: 2.0576, Perplexity: 7.82746\n",
      "Epoch [1/3], Step [36800/41412], Loss: 1.8152, Perplexity: 6.14250\n",
      "Epoch [1/3], Step [36900/41412], Loss: 2.0483, Perplexity: 7.75483\n",
      "Epoch [1/3], Step [37000/41412], Loss: 2.5633, Perplexity: 12.9784\n",
      "Epoch [1/3], Step [37100/41412], Loss: 2.0904, Perplexity: 8.08820\n",
      "Epoch [1/3], Step [37200/41412], Loss: 2.3805, Perplexity: 10.8104\n",
      "Epoch [1/3], Step [37300/41412], Loss: 2.0917, Perplexity: 8.09900\n",
      "Epoch [1/3], Step [37400/41412], Loss: 2.5046, Perplexity: 12.2383\n",
      "Epoch [1/3], Step [37500/41412], Loss: 2.8048, Perplexity: 16.5234\n",
      "Epoch [1/3], Step [37600/41412], Loss: 2.2360, Perplexity: 9.35580\n",
      "Epoch [1/3], Step [37700/41412], Loss: 2.3575, Perplexity: 10.5644\n",
      "Epoch [1/3], Step [37800/41412], Loss: 2.6401, Perplexity: 14.0140\n",
      "Epoch [1/3], Step [37900/41412], Loss: 2.3718, Perplexity: 10.7168\n",
      "Epoch [1/3], Step [38000/41412], Loss: 2.2912, Perplexity: 9.88723\n",
      "Epoch [1/3], Step [38100/41412], Loss: 2.0743, Perplexity: 7.95869\n",
      "Epoch [1/3], Step [38200/41412], Loss: 2.1360, Perplexity: 8.46532\n",
      "Epoch [1/3], Step [38300/41412], Loss: 2.5049, Perplexity: 12.2428\n",
      "Epoch [1/3], Step [38400/41412], Loss: 2.3603, Perplexity: 10.5943\n",
      "Epoch [1/3], Step [38500/41412], Loss: 2.2572, Perplexity: 9.55629\n",
      "Epoch [1/3], Step [38600/41412], Loss: 2.2251, Perplexity: 9.25422\n",
      "Epoch [1/3], Step [38700/41412], Loss: 2.4011, Perplexity: 11.0350\n",
      "Epoch [1/3], Step [38800/41412], Loss: 2.2706, Perplexity: 9.68563\n",
      "Epoch [1/3], Step [38900/41412], Loss: 2.2638, Perplexity: 9.61996\n",
      "Epoch [1/3], Step [39000/41412], Loss: 2.2296, Perplexity: 9.29585\n",
      "Epoch [1/3], Step [39100/41412], Loss: 2.0420, Perplexity: 7.70647\n",
      "Epoch [1/3], Step [39200/41412], Loss: 1.8427, Perplexity: 6.31345\n",
      "Epoch [1/3], Step [39300/41412], Loss: 2.5769, Perplexity: 13.1559\n",
      "Epoch [1/3], Step [39400/41412], Loss: 2.7793, Perplexity: 16.1081\n",
      "Epoch [1/3], Step [39500/41412], Loss: 2.4502, Perplexity: 11.5907\n",
      "Epoch [1/3], Step [39600/41412], Loss: 2.4174, Perplexity: 11.2163\n",
      "Epoch [1/3], Step [39700/41412], Loss: 1.9785, Perplexity: 7.23204\n",
      "Epoch [1/3], Step [39800/41412], Loss: 2.2800, Perplexity: 9.77690\n",
      "Epoch [1/3], Step [39900/41412], Loss: 2.5272, Perplexity: 12.5179\n",
      "Epoch [1/3], Step [40000/41412], Loss: 2.8396, Perplexity: 17.1092\n",
      "Epoch [1/3], Step [40100/41412], Loss: 2.4593, Perplexity: 11.6963\n",
      "Epoch [1/3], Step [40200/41412], Loss: 2.2283, Perplexity: 9.28390\n",
      "Epoch [1/3], Step [40300/41412], Loss: 2.6397, Perplexity: 14.0096\n",
      "Epoch [1/3], Step [40400/41412], Loss: 2.3203, Perplexity: 10.1786\n",
      "Epoch [1/3], Step [40500/41412], Loss: 2.8767, Perplexity: 17.7560\n",
      "Epoch [1/3], Step [40600/41412], Loss: 2.6293, Perplexity: 13.8646\n",
      "Epoch [1/3], Step [40700/41412], Loss: 2.1585, Perplexity: 8.65813\n",
      "Epoch [1/3], Step [40800/41412], Loss: 2.0831, Perplexity: 8.02901\n",
      "Epoch [1/3], Step [40900/41412], Loss: 2.5604, Perplexity: 12.9415\n",
      "Epoch [1/3], Step [41000/41412], Loss: 2.5901, Perplexity: 13.3309\n",
      "Epoch [1/3], Step [41100/41412], Loss: 2.2569, Perplexity: 9.55398\n",
      "Epoch [1/3], Step [41200/41412], Loss: 2.0029, Perplexity: 7.41060\n",
      "Epoch [1/3], Step [41300/41412], Loss: 2.2447, Perplexity: 9.43791\n",
      "Epoch [1/3], Step [41400/41412], Loss: 2.8656, Perplexity: 17.5595\n",
      "Epoch [2/3], Step [100/41412], Loss: 2.7387, Perplexity: 15.467617\n",
      "Epoch [2/3], Step [200/41412], Loss: 2.0197, Perplexity: 7.53633\n",
      "Epoch [2/3], Step [300/41412], Loss: 2.4748, Perplexity: 11.8791\n",
      "Epoch [2/3], Step [400/41412], Loss: 2.1580, Perplexity: 8.65374\n",
      "Epoch [2/3], Step [500/41412], Loss: 3.1334, Perplexity: 22.9525\n",
      "Epoch [2/3], Step [600/41412], Loss: 2.2596, Perplexity: 9.57929\n",
      "Epoch [2/3], Step [700/41412], Loss: 2.1474, Perplexity: 8.56280\n",
      "Epoch [2/3], Step [800/41412], Loss: 2.4154, Perplexity: 11.1940\n",
      "Epoch [2/3], Step [900/41412], Loss: 2.4914, Perplexity: 12.0781\n",
      "Epoch [2/3], Step [1000/41412], Loss: 2.5664, Perplexity: 13.0183\n",
      "Epoch [2/3], Step [1100/41412], Loss: 2.8420, Perplexity: 17.1496\n",
      "Epoch [2/3], Step [1200/41412], Loss: 2.5693, Perplexity: 13.0565\n",
      "Epoch [2/3], Step [1300/41412], Loss: 2.3504, Perplexity: 10.4898\n",
      "Epoch [2/3], Step [1400/41412], Loss: 2.5530, Perplexity: 12.8459\n",
      "Epoch [2/3], Step [1500/41412], Loss: 2.5975, Perplexity: 13.4297\n",
      "Epoch [2/3], Step [1600/41412], Loss: 2.7125, Perplexity: 15.0667\n",
      "Epoch [2/3], Step [1700/41412], Loss: 1.9936, Perplexity: 7.34194\n",
      "Epoch [2/3], Step [1800/41412], Loss: 2.2625, Perplexity: 9.60670\n",
      "Epoch [2/3], Step [1900/41412], Loss: 2.9086, Perplexity: 18.3302\n",
      "Epoch [2/3], Step [2000/41412], Loss: 2.5732, Perplexity: 13.1079\n",
      "Epoch [2/3], Step [2100/41412], Loss: 2.1822, Perplexity: 8.86552\n",
      "Epoch [2/3], Step [2200/41412], Loss: 2.2063, Perplexity: 9.08181\n",
      "Epoch [2/3], Step [2300/41412], Loss: 2.3534, Perplexity: 10.5216\n",
      "Epoch [2/3], Step [2400/41412], Loss: 2.1423, Perplexity: 8.51913\n",
      "Epoch [2/3], Step [2500/41412], Loss: 2.2916, Perplexity: 9.89053\n",
      "Epoch [2/3], Step [2600/41412], Loss: 2.0242, Perplexity: 7.57024\n",
      "Epoch [2/3], Step [2700/41412], Loss: 2.0840, Perplexity: 8.03692\n",
      "Epoch [2/3], Step [2800/41412], Loss: 2.9431, Perplexity: 18.9748\n",
      "Epoch [2/3], Step [2900/41412], Loss: 2.6518, Perplexity: 14.1797\n",
      "Epoch [2/3], Step [3000/41412], Loss: 2.0961, Perplexity: 8.13440\n",
      "Epoch [2/3], Step [3100/41412], Loss: 2.1134, Perplexity: 8.27620\n",
      "Epoch [2/3], Step [3200/41412], Loss: 2.5180, Perplexity: 12.4039\n",
      "Epoch [2/3], Step [3300/41412], Loss: 2.4098, Perplexity: 11.1317\n",
      "Epoch [2/3], Step [3400/41412], Loss: 1.7391, Perplexity: 5.69235\n",
      "Epoch [2/3], Step [3500/41412], Loss: 2.3104, Perplexity: 10.0788\n",
      "Epoch [2/3], Step [3600/41412], Loss: 1.8148, Perplexity: 6.13964\n",
      "Epoch [2/3], Step [3700/41412], Loss: 2.4474, Perplexity: 11.55881\n",
      "Epoch [2/3], Step [3800/41412], Loss: 2.0671, Perplexity: 7.90166\n",
      "Epoch [2/3], Step [3900/41412], Loss: 2.1615, Perplexity: 8.68370\n",
      "Epoch [2/3], Step [4000/41412], Loss: 2.5418, Perplexity: 12.7025\n",
      "Epoch [2/3], Step [4100/41412], Loss: 2.0575, Perplexity: 7.82627\n",
      "Epoch [2/3], Step [4200/41412], Loss: 2.2290, Perplexity: 9.29104\n",
      "Epoch [2/3], Step [4300/41412], Loss: 2.2091, Perplexity: 9.10739\n",
      "Epoch [2/3], Step [4400/41412], Loss: 2.3925, Perplexity: 10.9408\n",
      "Epoch [2/3], Step [4500/41412], Loss: 2.0311, Perplexity: 7.62274\n",
      "Epoch [2/3], Step [4600/41412], Loss: 2.2630, Perplexity: 9.61189\n",
      "Epoch [2/3], Step [4700/41412], Loss: 2.4757, Perplexity: 11.8900\n",
      "Epoch [2/3], Step [4800/41412], Loss: 2.3496, Perplexity: 10.4812\n",
      "Epoch [2/3], Step [4900/41412], Loss: 2.3383, Perplexity: 10.3640\n",
      "Epoch [2/3], Step [5000/41412], Loss: 2.9075, Perplexity: 18.3114\n",
      "Epoch [2/3], Step [5100/41412], Loss: 1.8587, Perplexity: 6.41529\n",
      "Epoch [2/3], Step [5200/41412], Loss: 1.7951, Perplexity: 6.02035\n",
      "Epoch [2/3], Step [5300/41412], Loss: 2.6283, Perplexity: 13.8507\n",
      "Epoch [2/3], Step [5400/41412], Loss: 2.3775, Perplexity: 10.7783\n",
      "Epoch [2/3], Step [5500/41412], Loss: 2.4872, Perplexity: 12.0272\n",
      "Epoch [2/3], Step [5600/41412], Loss: 2.0828, Perplexity: 8.02691\n",
      "Epoch [2/3], Step [5700/41412], Loss: 2.2317, Perplexity: 9.315824\n",
      "Epoch [2/3], Step [5800/41412], Loss: 2.0538, Perplexity: 7.79775\n",
      "Epoch [2/3], Step [5900/41412], Loss: 1.9406, Perplexity: 6.96293\n",
      "Epoch [2/3], Step [6000/41412], Loss: 2.5039, Perplexity: 12.2300\n",
      "Epoch [2/3], Step [6100/41412], Loss: 2.4754, Perplexity: 11.8859\n",
      "Epoch [2/3], Step [6200/41412], Loss: 2.0102, Perplexity: 7.46451\n",
      "Epoch [2/3], Step [6300/41412], Loss: 2.4346, Perplexity: 11.4107\n",
      "Epoch [2/3], Step [6400/41412], Loss: 2.2762, Perplexity: 9.73962\n",
      "Epoch [2/3], Step [6500/41412], Loss: 2.4295, Perplexity: 11.3533\n",
      "Epoch [2/3], Step [6600/41412], Loss: 2.1433, Perplexity: 8.52785\n",
      "Epoch [2/3], Step [6700/41412], Loss: 2.5006, Perplexity: 12.1903\n",
      "Epoch [2/3], Step [6800/41412], Loss: 2.6337, Perplexity: 13.9254\n",
      "Epoch [2/3], Step [6900/41412], Loss: 2.0846, Perplexity: 8.04142\n",
      "Epoch [2/3], Step [7000/41412], Loss: 1.7919, Perplexity: 6.00093\n",
      "Epoch [2/3], Step [7100/41412], Loss: 2.6111, Perplexity: 13.6146\n",
      "Epoch [2/3], Step [7200/41412], Loss: 2.3265, Perplexity: 10.2420\n",
      "Epoch [2/3], Step [7300/41412], Loss: 2.3267, Perplexity: 10.2444\n",
      "Epoch [2/3], Step [7400/41412], Loss: 2.4302, Perplexity: 11.3616\n",
      "Epoch [2/3], Step [7500/41412], Loss: 2.1766, Perplexity: 8.816558\n",
      "Epoch [2/3], Step [7600/41412], Loss: 2.5871, Perplexity: 13.2916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [7700/41412], Loss: 2.1660, Perplexity: 8.72350\n",
      "Epoch [2/3], Step [7800/41412], Loss: 1.8076, Perplexity: 6.09617\n",
      "Epoch [2/3], Step [7900/41412], Loss: 2.1157, Perplexity: 8.29583\n",
      "Epoch [2/3], Step [8000/41412], Loss: 2.3574, Perplexity: 10.5637\n",
      "Epoch [2/3], Step [8100/41412], Loss: 1.8706, Perplexity: 6.49192\n",
      "Epoch [2/3], Step [8200/41412], Loss: 2.2369, Perplexity: 9.36410\n",
      "Epoch [2/3], Step [8300/41412], Loss: 2.4201, Perplexity: 11.24693\n",
      "Epoch [2/3], Step [8400/41412], Loss: 2.3064, Perplexity: 10.0381\n",
      "Epoch [2/3], Step [8500/41412], Loss: 2.7628, Perplexity: 15.8436\n",
      "Epoch [2/3], Step [8600/41412], Loss: 2.4618, Perplexity: 11.7253\n",
      "Epoch [2/3], Step [8700/41412], Loss: 2.2161, Perplexity: 9.17196\n",
      "Epoch [2/3], Step [8800/41412], Loss: 2.2620, Perplexity: 9.60276\n",
      "Epoch [2/3], Step [8900/41412], Loss: 2.2665, Perplexity: 9.64558\n",
      "Epoch [2/3], Step [9000/41412], Loss: 2.5345, Perplexity: 12.6107\n",
      "Epoch [2/3], Step [9100/41412], Loss: 1.7856, Perplexity: 5.96347\n",
      "Epoch [2/3], Step [9200/41412], Loss: 3.0289, Perplexity: 20.6741\n",
      "Epoch [2/3], Step [9300/41412], Loss: 3.1246, Perplexity: 22.7511\n",
      "Epoch [2/3], Step [9400/41412], Loss: 2.3196, Perplexity: 10.1717\n",
      "Epoch [2/3], Step [9500/41412], Loss: 2.5459, Perplexity: 12.7548\n",
      "Epoch [2/3], Step [9600/41412], Loss: 2.3845, Perplexity: 10.8531\n",
      "Epoch [2/3], Step [9700/41412], Loss: 3.2652, Perplexity: 26.1851\n",
      "Epoch [2/3], Step [9800/41412], Loss: 2.1706, Perplexity: 8.76387\n",
      "Epoch [2/3], Step [9900/41412], Loss: 2.5003, Perplexity: 12.1863\n",
      "Epoch [2/3], Step [10000/41412], Loss: 2.2686, Perplexity: 9.6657\n",
      "Epoch [2/3], Step [10100/41412], Loss: 2.3063, Perplexity: 10.0371\n",
      "Epoch [2/3], Step [10200/41412], Loss: 2.0843, Perplexity: 8.03883\n",
      "Epoch [2/3], Step [10300/41412], Loss: 2.4869, Perplexity: 12.0241\n",
      "Epoch [2/3], Step [10400/41412], Loss: 2.3461, Perplexity: 10.4452\n",
      "Epoch [2/3], Step [10500/41412], Loss: 2.3815, Perplexity: 10.82080\n",
      "Epoch [2/3], Step [10600/41412], Loss: 2.2917, Perplexity: 9.89175\n",
      "Epoch [2/3], Step [10700/41412], Loss: 2.4775, Perplexity: 11.9115\n",
      "Epoch [2/3], Step [10800/41412], Loss: 2.3032, Perplexity: 10.0063\n",
      "Epoch [2/3], Step [10900/41412], Loss: 1.9941, Perplexity: 7.34572\n",
      "Epoch [2/3], Step [11000/41412], Loss: 2.5274, Perplexity: 12.5210\n",
      "Epoch [2/3], Step [11100/41412], Loss: 2.7854, Perplexity: 16.2066\n",
      "Epoch [2/3], Step [11200/41412], Loss: 2.2248, Perplexity: 9.25216\n",
      "Epoch [2/3], Step [11300/41412], Loss: 1.8919, Perplexity: 6.63182\n",
      "Epoch [2/3], Step [11400/41412], Loss: 2.3643, Perplexity: 10.6361\n",
      "Epoch [2/3], Step [11500/41412], Loss: 1.9515, Perplexity: 7.03903\n",
      "Epoch [2/3], Step [11600/41412], Loss: 2.6532, Perplexity: 14.1987\n",
      "Epoch [2/3], Step [11700/41412], Loss: 2.3200, Perplexity: 10.1756\n",
      "Epoch [2/3], Step [11800/41412], Loss: 2.6167, Perplexity: 13.6911\n",
      "Epoch [2/3], Step [11900/41412], Loss: 2.1919, Perplexity: 8.95263\n",
      "Epoch [2/3], Step [12000/41412], Loss: 2.5376, Perplexity: 12.6491\n",
      "Epoch [2/3], Step [12100/41412], Loss: 1.9606, Perplexity: 7.10330\n",
      "Epoch [2/3], Step [12200/41412], Loss: 3.0576, Perplexity: 21.2758\n",
      "Epoch [2/3], Step [12300/41412], Loss: 1.9975, Perplexity: 7.37036\n",
      "Epoch [2/3], Step [12400/41412], Loss: 2.3755, Perplexity: 10.7565\n",
      "Epoch [2/3], Step [12500/41412], Loss: 1.9948, Perplexity: 7.35062\n",
      "Epoch [2/3], Step [12600/41412], Loss: 1.7028, Perplexity: 5.48923\n",
      "Epoch [2/3], Step [12700/41412], Loss: 2.1522, Perplexity: 8.60404\n",
      "Epoch [2/3], Step [12800/41412], Loss: 2.2011, Perplexity: 9.03529\n",
      "Epoch [2/3], Step [12900/41412], Loss: 2.1655, Perplexity: 8.71927\n",
      "Epoch [2/3], Step [13000/41412], Loss: 2.2238, Perplexity: 9.24284\n",
      "Epoch [2/3], Step [13100/41412], Loss: 2.3891, Perplexity: 10.9033\n",
      "Epoch [2/3], Step [13200/41412], Loss: 2.1801, Perplexity: 8.84706\n",
      "Epoch [2/3], Step [13300/41412], Loss: 2.5626, Perplexity: 12.9693\n",
      "Epoch [2/3], Step [13400/41412], Loss: 2.4422, Perplexity: 11.4978\n",
      "Epoch [2/3], Step [13500/41412], Loss: 2.7183, Perplexity: 15.1539\n",
      "Epoch [2/3], Step [13600/41412], Loss: 2.3019, Perplexity: 9.99316\n",
      "Epoch [2/3], Step [13700/41412], Loss: 2.6181, Perplexity: 13.7101\n",
      "Epoch [2/3], Step [13800/41412], Loss: 2.3101, Perplexity: 10.0757\n",
      "Epoch [2/3], Step [13900/41412], Loss: 2.2096, Perplexity: 9.11200\n",
      "Epoch [2/3], Step [14000/41412], Loss: 2.2141, Perplexity: 9.15284\n",
      "Epoch [2/3], Step [14100/41412], Loss: 1.6709, Perplexity: 5.31684\n",
      "Epoch [2/3], Step [14200/41412], Loss: 2.0745, Perplexity: 7.96041\n",
      "Epoch [2/3], Step [14300/41412], Loss: 2.0178, Perplexity: 7.52191\n",
      "Epoch [2/3], Step [14400/41412], Loss: 2.3055, Perplexity: 10.0293\n",
      "Epoch [2/3], Step [14500/41412], Loss: 1.6438, Perplexity: 5.17501\n",
      "Epoch [2/3], Step [14600/41412], Loss: 2.3805, Perplexity: 10.8099\n",
      "Epoch [2/3], Step [14700/41412], Loss: 2.8495, Perplexity: 17.2794\n",
      "Epoch [2/3], Step [14800/41412], Loss: 2.0262, Perplexity: 7.58505\n",
      "Epoch [2/3], Step [14900/41412], Loss: 2.2624, Perplexity: 9.60581\n",
      "Epoch [2/3], Step [15000/41412], Loss: 2.1822, Perplexity: 8.86574\n",
      "Epoch [2/3], Step [15100/41412], Loss: 2.2832, Perplexity: 9.80815\n",
      "Epoch [2/3], Step [15200/41412], Loss: 2.6848, Perplexity: 14.6558\n",
      "Epoch [2/3], Step [15300/41412], Loss: 1.9127, Perplexity: 6.77135\n",
      "Epoch [2/3], Step [15400/41412], Loss: 2.0257, Perplexity: 7.58114\n",
      "Epoch [2/3], Step [15500/41412], Loss: 2.2212, Perplexity: 9.21856\n",
      "Epoch [2/3], Step [15600/41412], Loss: 1.9760, Perplexity: 7.21377\n",
      "Epoch [2/3], Step [15700/41412], Loss: 2.7316, Perplexity: 15.3574\n",
      "Epoch [2/3], Step [15800/41412], Loss: 2.4555, Perplexity: 11.6520\n",
      "Epoch [2/3], Step [15900/41412], Loss: 2.2054, Perplexity: 9.07411\n",
      "Epoch [2/3], Step [16000/41412], Loss: 2.9860, Perplexity: 19.8060\n",
      "Epoch [2/3], Step [16100/41412], Loss: 2.0133, Perplexity: 7.487721\n",
      "Epoch [2/3], Step [16200/41412], Loss: 2.1598, Perplexity: 8.66981\n",
      "Epoch [2/3], Step [16300/41412], Loss: 1.7607, Perplexity: 5.81687\n",
      "Epoch [2/3], Step [16400/41412], Loss: 2.5187, Perplexity: 12.4120\n",
      "Epoch [2/3], Step [16500/41412], Loss: 2.3890, Perplexity: 10.9028\n",
      "Epoch [2/3], Step [16600/41412], Loss: 2.4525, Perplexity: 11.6176\n",
      "Epoch [2/3], Step [16700/41412], Loss: 2.1661, Perplexity: 8.72416\n",
      "Epoch [2/3], Step [16800/41412], Loss: 2.8529, Perplexity: 17.3373\n",
      "Epoch [2/3], Step [16900/41412], Loss: 2.3859, Perplexity: 10.8690\n",
      "Epoch [2/3], Step [17000/41412], Loss: 2.0451, Perplexity: 7.73012\n",
      "Epoch [2/3], Step [17100/41412], Loss: 3.0735, Perplexity: 21.6185\n",
      "Epoch [2/3], Step [17200/41412], Loss: 1.9102, Perplexity: 6.75412\n",
      "Epoch [2/3], Step [17300/41412], Loss: 2.3407, Perplexity: 10.3887\n",
      "Epoch [2/3], Step [17400/41412], Loss: 2.2979, Perplexity: 9.95367\n",
      "Epoch [2/3], Step [17500/41412], Loss: 2.3906, Perplexity: 10.9202\n",
      "Epoch [2/3], Step [17600/41412], Loss: 2.0634, Perplexity: 7.87242\n",
      "Epoch [2/3], Step [17700/41412], Loss: 2.2963, Perplexity: 9.93735\n",
      "Epoch [2/3], Step [17800/41412], Loss: 2.4010, Perplexity: 11.0340\n",
      "Epoch [2/3], Step [17900/41412], Loss: 2.2506, Perplexity: 9.49357\n",
      "Epoch [2/3], Step [18000/41412], Loss: 1.8944, Perplexity: 6.648860\n",
      "Epoch [2/3], Step [18100/41412], Loss: 2.3602, Perplexity: 10.5935\n",
      "Epoch [2/3], Step [18200/41412], Loss: 2.6039, Perplexity: 13.5168\n",
      "Epoch [2/3], Step [18300/41412], Loss: 2.6559, Perplexity: 14.2380\n",
      "Epoch [2/3], Step [18400/41412], Loss: 2.8421, Perplexity: 17.1515\n",
      "Epoch [2/3], Step [18500/41412], Loss: 2.0440, Perplexity: 7.72114\n",
      "Epoch [2/3], Step [18600/41412], Loss: 2.5809, Perplexity: 13.2090\n",
      "Epoch [2/3], Step [18700/41412], Loss: 2.7164, Perplexity: 15.1264\n",
      "Epoch [2/3], Step [18800/41412], Loss: 2.1385, Perplexity: 8.48718\n",
      "Epoch [2/3], Step [18900/41412], Loss: 1.7658, Perplexity: 5.84657\n",
      "Epoch [2/3], Step [19000/41412], Loss: 1.9971, Perplexity: 7.36801\n",
      "Epoch [2/3], Step [19100/41412], Loss: 3.8104, Perplexity: 45.1672\n",
      "Epoch [2/3], Step [19200/41412], Loss: 2.0209, Perplexity: 7.54525\n",
      "Epoch [2/3], Step [19300/41412], Loss: 2.3734, Perplexity: 10.7333\n",
      "Epoch [2/3], Step [19400/41412], Loss: 1.9549, Perplexity: 7.063506\n",
      "Epoch [2/3], Step [19500/41412], Loss: 2.7618, Perplexity: 15.8279\n",
      "Epoch [2/3], Step [19600/41412], Loss: 1.7850, Perplexity: 5.95969\n",
      "Epoch [2/3], Step [19700/41412], Loss: 2.6362, Perplexity: 13.9607\n",
      "Epoch [2/3], Step [19800/41412], Loss: 3.2291, Perplexity: 25.2563\n",
      "Epoch [2/3], Step [19900/41412], Loss: 2.4825, Perplexity: 11.9711\n",
      "Epoch [2/3], Step [20000/41412], Loss: 3.0540, Perplexity: 21.1989\n",
      "Epoch [2/3], Step [20100/41412], Loss: 1.9469, Perplexity: 7.00663\n",
      "Epoch [2/3], Step [20200/41412], Loss: 2.3420, Perplexity: 10.4018\n",
      "Epoch [2/3], Step [20300/41412], Loss: 2.5459, Perplexity: 12.7543\n",
      "Epoch [2/3], Step [20400/41412], Loss: 2.5189, Perplexity: 12.4146\n",
      "Epoch [2/3], Step [20500/41412], Loss: 1.9632, Perplexity: 7.12175\n",
      "Epoch [2/3], Step [20600/41412], Loss: 2.4617, Perplexity: 11.7242\n",
      "Epoch [2/3], Step [20700/41412], Loss: 2.4473, Perplexity: 11.5574\n",
      "Epoch [2/3], Step [20800/41412], Loss: 2.7446, Perplexity: 15.5582\n",
      "Epoch [2/3], Step [20900/41412], Loss: 2.1851, Perplexity: 8.89150\n",
      "Epoch [2/3], Step [21000/41412], Loss: 2.7625, Perplexity: 15.8386\n",
      "Epoch [2/3], Step [21100/41412], Loss: 2.3304, Perplexity: 10.2824\n",
      "Epoch [2/3], Step [21200/41412], Loss: 3.1393, Perplexity: 23.0866\n",
      "Epoch [2/3], Step [21300/41412], Loss: 2.2301, Perplexity: 9.30110\n",
      "Epoch [2/3], Step [21400/41412], Loss: 3.1076, Perplexity: 22.3682\n",
      "Epoch [2/3], Step [21500/41412], Loss: 2.1816, Perplexity: 8.86022\n",
      "Epoch [2/3], Step [21600/41412], Loss: 1.8938, Perplexity: 6.64458\n",
      "Epoch [2/3], Step [21700/41412], Loss: 2.7860, Perplexity: 16.2152\n",
      "Epoch [2/3], Step [21800/41412], Loss: 2.0094, Perplexity: 7.45905\n",
      "Epoch [2/3], Step [21900/41412], Loss: 2.5812, Perplexity: 13.2130\n",
      "Epoch [2/3], Step [22000/41412], Loss: 1.7416, Perplexity: 5.70647\n",
      "Epoch [2/3], Step [22100/41412], Loss: 2.2264, Perplexity: 9.26639\n",
      "Epoch [2/3], Step [22200/41412], Loss: 2.2633, Perplexity: 9.61509\n",
      "Epoch [2/3], Step [22300/41412], Loss: 2.4074, Perplexity: 11.10511\n",
      "Epoch [2/3], Step [22400/41412], Loss: 2.1052, Perplexity: 8.208801\n",
      "Epoch [2/3], Step [22500/41412], Loss: 2.2551, Perplexity: 9.53604\n",
      "Epoch [2/3], Step [22600/41412], Loss: 1.6483, Perplexity: 5.19836\n",
      "Epoch [2/3], Step [22700/41412], Loss: 2.1873, Perplexity: 8.91117\n",
      "Epoch [2/3], Step [22800/41412], Loss: 2.2431, Perplexity: 9.42236\n",
      "Epoch [2/3], Step [22900/41412], Loss: 2.3031, Perplexity: 10.0049\n",
      "Epoch [2/3], Step [23000/41412], Loss: 2.6933, Perplexity: 14.7796\n",
      "Epoch [2/3], Step [23100/41412], Loss: 2.4665, Perplexity: 11.7814\n",
      "Epoch [2/3], Step [23200/41412], Loss: 2.6768, Perplexity: 14.5382\n",
      "Epoch [2/3], Step [23300/41412], Loss: 1.9506, Perplexity: 7.03310\n",
      "Epoch [2/3], Step [23400/41412], Loss: 2.4137, Perplexity: 11.1754\n",
      "Epoch [2/3], Step [23500/41412], Loss: 1.6898, Perplexity: 5.41848\n",
      "Epoch [2/3], Step [23600/41412], Loss: 2.4736, Perplexity: 11.8657\n",
      "Epoch [2/3], Step [23700/41412], Loss: 1.8542, Perplexity: 6.38652\n",
      "Epoch [2/3], Step [23800/41412], Loss: 2.0984, Perplexity: 8.15282\n",
      "Epoch [2/3], Step [23900/41412], Loss: 2.1458, Perplexity: 8.54919\n",
      "Epoch [2/3], Step [24000/41412], Loss: 2.2702, Perplexity: 9.68155\n",
      "Epoch [2/3], Step [24100/41412], Loss: 1.8724, Perplexity: 6.50419\n",
      "Epoch [2/3], Step [24200/41412], Loss: 2.0957, Perplexity: 8.13127\n",
      "Epoch [2/3], Step [24300/41412], Loss: 1.8291, Perplexity: 6.22857\n",
      "Epoch [2/3], Step [24400/41412], Loss: 2.7912, Perplexity: 16.3010\n",
      "Epoch [2/3], Step [24500/41412], Loss: 1.9527, Perplexity: 7.04805\n",
      "Epoch [2/3], Step [24600/41412], Loss: 2.2846, Perplexity: 9.82209\n",
      "Epoch [2/3], Step [24700/41412], Loss: 2.5450, Perplexity: 12.7433\n",
      "Epoch [2/3], Step [24800/41412], Loss: 2.4599, Perplexity: 11.7033\n",
      "Epoch [2/3], Step [24900/41412], Loss: 2.3076, Perplexity: 10.0500\n",
      "Epoch [2/3], Step [25000/41412], Loss: 2.0390, Perplexity: 7.68286\n",
      "Epoch [2/3], Step [25100/41412], Loss: 1.9463, Perplexity: 7.00284\n",
      "Epoch [2/3], Step [25200/41412], Loss: 2.0611, Perplexity: 7.85472\n",
      "Epoch [2/3], Step [25300/41412], Loss: 2.3845, Perplexity: 10.8537\n",
      "Epoch [2/3], Step [25400/41412], Loss: 2.2375, Perplexity: 9.37035\n",
      "Epoch [2/3], Step [25500/41412], Loss: 2.5867, Perplexity: 13.2863\n",
      "Epoch [2/3], Step [25600/41412], Loss: 2.2503, Perplexity: 9.49060\n",
      "Epoch [2/3], Step [25700/41412], Loss: 1.9891, Perplexity: 7.30914\n",
      "Epoch [2/3], Step [25800/41412], Loss: 2.4403, Perplexity: 11.4766\n",
      "Epoch [2/3], Step [25900/41412], Loss: 2.7422, Perplexity: 15.5203\n",
      "Epoch [2/3], Step [26000/41412], Loss: 2.0768, Perplexity: 7.97901\n",
      "Epoch [2/3], Step [26100/41412], Loss: 1.9992, Perplexity: 7.38290\n",
      "Epoch [2/3], Step [26200/41412], Loss: 2.4422, Perplexity: 11.4988\n",
      "Epoch [2/3], Step [26300/41412], Loss: 1.2445, Perplexity: 3.47140\n",
      "Epoch [2/3], Step [26400/41412], Loss: 2.1012, Perplexity: 8.17564\n",
      "Epoch [2/3], Step [26500/41412], Loss: 2.1439, Perplexity: 8.53260\n",
      "Epoch [2/3], Step [26600/41412], Loss: 1.8225, Perplexity: 6.18745\n",
      "Epoch [2/3], Step [26700/41412], Loss: 2.3851, Perplexity: 10.8597\n",
      "Epoch [2/3], Step [26800/41412], Loss: 2.2175, Perplexity: 9.18477\n",
      "Epoch [2/3], Step [26900/41412], Loss: 2.0622, Perplexity: 7.86325\n",
      "Epoch [2/3], Step [27000/41412], Loss: 1.7618, Perplexity: 5.82320\n",
      "Epoch [2/3], Step [27100/41412], Loss: 2.0605, Perplexity: 7.85015\n",
      "Epoch [2/3], Step [27200/41412], Loss: 1.9721, Perplexity: 7.18602\n",
      "Epoch [2/3], Step [27300/41412], Loss: 2.4212, Perplexity: 11.2594\n",
      "Epoch [2/3], Step [27400/41412], Loss: 1.7429, Perplexity: 5.71404\n",
      "Epoch [2/3], Step [27500/41412], Loss: 2.3760, Perplexity: 10.7615\n",
      "Epoch [2/3], Step [27600/41412], Loss: 2.3886, Perplexity: 10.8982\n",
      "Epoch [2/3], Step [27700/41412], Loss: 1.8896, Perplexity: 6.61697\n",
      "Epoch [2/3], Step [27800/41412], Loss: 2.4008, Perplexity: 11.0325\n",
      "Epoch [2/3], Step [27900/41412], Loss: 1.9766, Perplexity: 7.21790\n",
      "Epoch [2/3], Step [28000/41412], Loss: 2.0611, Perplexity: 7.85438\n",
      "Epoch [2/3], Step [28100/41412], Loss: 1.9382, Perplexity: 6.94649\n",
      "Epoch [2/3], Step [28200/41412], Loss: 1.9318, Perplexity: 6.90225\n",
      "Epoch [2/3], Step [28300/41412], Loss: 2.5256, Perplexity: 12.4981\n",
      "Epoch [2/3], Step [28400/41412], Loss: 2.8044, Perplexity: 16.5174\n",
      "Epoch [2/3], Step [28500/41412], Loss: 2.4660, Perplexity: 11.7750\n",
      "Epoch [2/3], Step [28600/41412], Loss: 2.4874, Perplexity: 12.0300\n",
      "Epoch [2/3], Step [28700/41412], Loss: 2.5505, Perplexity: 12.8130\n",
      "Epoch [2/3], Step [28800/41412], Loss: 2.5435, Perplexity: 12.7241\n",
      "Epoch [2/3], Step [28900/41412], Loss: 2.7081, Perplexity: 15.0004\n",
      "Epoch [2/3], Step [29000/41412], Loss: 2.2111, Perplexity: 9.12587\n",
      "Epoch [2/3], Step [29100/41412], Loss: 2.4196, Perplexity: 11.2418\n",
      "Epoch [2/3], Step [29200/41412], Loss: 2.4319, Perplexity: 11.3805\n",
      "Epoch [2/3], Step [29300/41412], Loss: 2.6767, Perplexity: 14.5372\n",
      "Epoch [2/3], Step [29400/41412], Loss: 2.4395, Perplexity: 11.4677\n",
      "Epoch [2/3], Step [29500/41412], Loss: 2.3966, Perplexity: 10.9853\n",
      "Epoch [2/3], Step [29600/41412], Loss: 2.1715, Perplexity: 8.77126\n",
      "Epoch [2/3], Step [29700/41412], Loss: 2.3110, Perplexity: 10.0842\n",
      "Epoch [2/3], Step [29800/41412], Loss: 2.1011, Perplexity: 8.17491\n",
      "Epoch [2/3], Step [29900/41412], Loss: 2.4854, Perplexity: 12.0054\n",
      "Epoch [2/3], Step [30000/41412], Loss: 2.0609, Perplexity: 7.85284\n",
      "Epoch [2/3], Step [30100/41412], Loss: 2.6055, Perplexity: 13.5386\n",
      "Epoch [2/3], Step [30200/41412], Loss: 2.7126, Perplexity: 15.0688\n",
      "Epoch [2/3], Step [30300/41412], Loss: 2.0301, Perplexity: 7.61474\n",
      "Epoch [2/3], Step [30400/41412], Loss: 2.8689, Perplexity: 17.6180\n",
      "Epoch [2/3], Step [30500/41412], Loss: 2.2429, Perplexity: 9.42072\n",
      "Epoch [2/3], Step [30600/41412], Loss: 2.4076, Perplexity: 11.1068\n",
      "Epoch [2/3], Step [30700/41412], Loss: 2.4391, Perplexity: 11.4626\n",
      "Epoch [2/3], Step [30800/41412], Loss: 2.6789, Perplexity: 14.5692\n",
      "Epoch [2/3], Step [30900/41412], Loss: 2.1724, Perplexity: 8.77970\n",
      "Epoch [2/3], Step [31000/41412], Loss: 3.4296, Perplexity: 30.8650\n",
      "Epoch [2/3], Step [31100/41412], Loss: 2.4469, Perplexity: 11.5525\n",
      "Epoch [2/3], Step [31200/41412], Loss: 1.8977, Perplexity: 6.67058\n",
      "Epoch [2/3], Step [31300/41412], Loss: 2.4626, Perplexity: 11.7347\n",
      "Epoch [2/3], Step [31400/41412], Loss: 2.3403, Perplexity: 10.3845\n",
      "Epoch [2/3], Step [31500/41412], Loss: 2.8851, Perplexity: 17.9051\n",
      "Epoch [2/3], Step [31600/41412], Loss: 3.6129, Perplexity: 37.0739\n",
      "Epoch [2/3], Step [31700/41412], Loss: 1.9882, Perplexity: 7.30238\n",
      "Epoch [2/3], Step [31800/41412], Loss: 2.1746, Perplexity: 8.79902\n",
      "Epoch [2/3], Step [31900/41412], Loss: 2.3318, Perplexity: 10.2965\n",
      "Epoch [2/3], Step [32000/41412], Loss: 2.6765, Perplexity: 14.5336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [32100/41412], Loss: 2.3916, Perplexity: 10.9314\n",
      "Epoch [2/3], Step [32200/41412], Loss: 2.0705, Perplexity: 7.92914\n",
      "Epoch [2/3], Step [32300/41412], Loss: 2.0191, Perplexity: 7.53161\n",
      "Epoch [2/3], Step [32400/41412], Loss: 2.9614, Perplexity: 19.3253\n",
      "Epoch [2/3], Step [32500/41412], Loss: 2.0338, Perplexity: 7.64346\n",
      "Epoch [2/3], Step [32600/41412], Loss: 2.1434, Perplexity: 8.52850\n",
      "Epoch [2/3], Step [32700/41412], Loss: 2.5693, Perplexity: 13.0573\n",
      "Epoch [2/3], Step [32800/41412], Loss: 2.3739, Perplexity: 10.7395\n",
      "Epoch [2/3], Step [32900/41412], Loss: 2.1707, Perplexity: 8.76429\n",
      "Epoch [2/3], Step [33000/41412], Loss: 2.1805, Perplexity: 8.85068\n",
      "Epoch [2/3], Step [33100/41412], Loss: 2.1485, Perplexity: 8.57217\n",
      "Epoch [2/3], Step [33200/41412], Loss: 2.3559, Perplexity: 10.5477\n",
      "Epoch [2/3], Step [33300/41412], Loss: 2.0779, Perplexity: 7.98782\n",
      "Epoch [2/3], Step [33400/41412], Loss: 2.3659, Perplexity: 10.6538\n",
      "Epoch [2/3], Step [33500/41412], Loss: 2.0117, Perplexity: 7.47645\n",
      "Epoch [2/3], Step [33600/41412], Loss: 2.5268, Perplexity: 12.5134\n",
      "Epoch [2/3], Step [33700/41412], Loss: 2.5956, Perplexity: 13.4049\n",
      "Epoch [2/3], Step [33800/41412], Loss: 2.0312, Perplexity: 7.62296\n",
      "Epoch [2/3], Step [33900/41412], Loss: 2.0627, Perplexity: 7.86738\n",
      "Epoch [2/3], Step [34000/41412], Loss: 2.0093, Perplexity: 7.45785\n",
      "Epoch [2/3], Step [34100/41412], Loss: 2.3369, Perplexity: 10.3486\n",
      "Epoch [2/3], Step [34200/41412], Loss: 2.4108, Perplexity: 11.1425\n",
      "Epoch [2/3], Step [34300/41412], Loss: 1.8599, Perplexity: 6.42324\n",
      "Epoch [2/3], Step [34400/41412], Loss: 2.1421, Perplexity: 8.51725\n",
      "Epoch [2/3], Step [34500/41412], Loss: 2.3017, Perplexity: 9.99074\n",
      "Epoch [2/3], Step [34600/41412], Loss: 2.1251, Perplexity: 8.37376\n",
      "Epoch [2/3], Step [34700/41412], Loss: 2.3552, Perplexity: 10.5402\n",
      "Epoch [2/3], Step [34800/41412], Loss: 2.1610, Perplexity: 8.67955\n",
      "Epoch [2/3], Step [34900/41412], Loss: 2.0567, Perplexity: 7.81991\n",
      "Epoch [2/3], Step [35000/41412], Loss: 2.6109, Perplexity: 13.6114\n",
      "Epoch [2/3], Step [35100/41412], Loss: 2.4099, Perplexity: 11.1333\n",
      "Epoch [2/3], Step [35200/41412], Loss: 2.3799, Perplexity: 10.8034\n",
      "Epoch [2/3], Step [35300/41412], Loss: 2.1197, Perplexity: 8.32837\n",
      "Epoch [2/3], Step [35400/41412], Loss: 2.5795, Perplexity: 13.1911\n",
      "Epoch [2/3], Step [35500/41412], Loss: 2.3098, Perplexity: 10.0721\n",
      "Epoch [2/3], Step [35600/41412], Loss: 2.3601, Perplexity: 10.5915\n",
      "Epoch [2/3], Step [35700/41412], Loss: 2.2019, Perplexity: 9.04198\n",
      "Epoch [2/3], Step [35800/41412], Loss: 2.1033, Perplexity: 8.19288\n",
      "Epoch [2/3], Step [35900/41412], Loss: 2.8772, Perplexity: 17.7642\n",
      "Epoch [2/3], Step [36000/41412], Loss: 2.6179, Perplexity: 13.7069\n",
      "Epoch [2/3], Step [36100/41412], Loss: 2.2423, Perplexity: 9.41507\n",
      "Epoch [2/3], Step [36200/41412], Loss: 1.9727, Perplexity: 7.189920\n",
      "Epoch [2/3], Step [36300/41412], Loss: 2.3715, Perplexity: 10.7130\n",
      "Epoch [2/3], Step [36400/41412], Loss: 1.8864, Perplexity: 6.59575\n",
      "Epoch [2/3], Step [36500/41412], Loss: 2.1433, Perplexity: 8.52781\n",
      "Epoch [2/3], Step [36600/41412], Loss: 2.1412, Perplexity: 8.50987\n",
      "Epoch [2/3], Step [36700/41412], Loss: 2.3192, Perplexity: 10.1677\n",
      "Epoch [2/3], Step [36800/41412], Loss: 2.8611, Perplexity: 17.4802\n",
      "Epoch [2/3], Step [36900/41412], Loss: 2.3953, Perplexity: 10.9713\n",
      "Epoch [2/3], Step [37000/41412], Loss: 2.3677, Perplexity: 10.6731\n",
      "Epoch [2/3], Step [37100/41412], Loss: 2.1356, Perplexity: 8.46193\n",
      "Epoch [2/3], Step [37200/41412], Loss: 2.0163, Perplexity: 7.51056\n",
      "Epoch [2/3], Step [37300/41412], Loss: 2.8948, Perplexity: 18.0793\n",
      "Epoch [2/3], Step [37400/41412], Loss: 2.0925, Perplexity: 8.10498\n",
      "Epoch [2/3], Step [37500/41412], Loss: 2.0718, Perplexity: 7.93902\n",
      "Epoch [2/3], Step [37600/41412], Loss: 2.3483, Perplexity: 10.4673\n",
      "Epoch [2/3], Step [37700/41412], Loss: 2.4834, Perplexity: 11.9823\n",
      "Epoch [2/3], Step [37800/41412], Loss: 2.2651, Perplexity: 9.63259\n",
      "Epoch [2/3], Step [37900/41412], Loss: 2.3928, Perplexity: 10.9444\n",
      "Epoch [2/3], Step [38000/41412], Loss: 2.8061, Perplexity: 16.5454\n",
      "Epoch [2/3], Step [38100/41412], Loss: 1.8782, Perplexity: 6.54152\n",
      "Epoch [2/3], Step [38200/41412], Loss: 2.9154, Perplexity: 18.4566\n",
      "Epoch [2/3], Step [38300/41412], Loss: 2.2578, Perplexity: 9.56194\n",
      "Epoch [2/3], Step [38400/41412], Loss: 2.2898, Perplexity: 9.87342\n",
      "Epoch [2/3], Step [38500/41412], Loss: 2.0157, Perplexity: 7.50612\n",
      "Epoch [2/3], Step [38600/41412], Loss: 2.1859, Perplexity: 8.89879\n",
      "Epoch [2/3], Step [38700/41412], Loss: 2.0044, Perplexity: 7.42192\n",
      "Epoch [2/3], Step [38800/41412], Loss: 2.3326, Perplexity: 10.3046\n",
      "Epoch [2/3], Step [38900/41412], Loss: 2.5751, Perplexity: 13.1325\n",
      "Epoch [2/3], Step [39000/41412], Loss: 2.3487, Perplexity: 10.47239\n",
      "Epoch [2/3], Step [39100/41412], Loss: 2.1863, Perplexity: 8.90212\n",
      "Epoch [2/3], Step [39200/41412], Loss: 2.2541, Perplexity: 9.52698\n",
      "Epoch [2/3], Step [39300/41412], Loss: 2.7120, Perplexity: 15.0588\n",
      "Epoch [2/3], Step [39400/41412], Loss: 2.0957, Perplexity: 8.13090\n",
      "Epoch [2/3], Step [39500/41412], Loss: 2.0198, Perplexity: 7.53669\n",
      "Epoch [2/3], Step [39600/41412], Loss: 2.1081, Perplexity: 8.23307\n",
      "Epoch [2/3], Step [39700/41412], Loss: 2.6028, Perplexity: 13.5017\n",
      "Epoch [2/3], Step [39800/41412], Loss: 1.8904, Perplexity: 6.62186\n",
      "Epoch [2/3], Step [39900/41412], Loss: 2.2744, Perplexity: 9.72172\n",
      "Epoch [2/3], Step [40000/41412], Loss: 2.3696, Perplexity: 10.6932\n",
      "Epoch [2/3], Step [40100/41412], Loss: 1.9939, Perplexity: 7.34451\n",
      "Epoch [2/3], Step [40200/41412], Loss: 3.3817, Perplexity: 29.4198\n",
      "Epoch [2/3], Step [40300/41412], Loss: 2.1016, Perplexity: 8.17880\n",
      "Epoch [2/3], Step [40400/41412], Loss: 2.3317, Perplexity: 10.2950\n",
      "Epoch [2/3], Step [40500/41412], Loss: 1.9994, Perplexity: 7.38478\n",
      "Epoch [2/3], Step [40600/41412], Loss: 2.4765, Perplexity: 11.8997\n",
      "Epoch [2/3], Step [40700/41412], Loss: 1.9907, Perplexity: 7.32070\n",
      "Epoch [2/3], Step [40800/41412], Loss: 2.2778, Perplexity: 9.75543\n",
      "Epoch [2/3], Step [40900/41412], Loss: 2.5560, Perplexity: 12.8840\n",
      "Epoch [2/3], Step [41000/41412], Loss: 2.3004, Perplexity: 9.97823\n",
      "Epoch [2/3], Step [41100/41412], Loss: 2.4301, Perplexity: 11.3598\n",
      "Epoch [2/3], Step [41200/41412], Loss: 1.9114, Perplexity: 6.76271\n",
      "Epoch [2/3], Step [41300/41412], Loss: 1.6768, Perplexity: 5.34840\n",
      "Epoch [2/3], Step [41400/41412], Loss: 2.2862, Perplexity: 9.83743\n",
      "Epoch [3/3], Step [18/41412], Loss: 2.0497, Perplexity: 7.76527666"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-191c08218022>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Get training statistics.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Print training statistics (on same line).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "            \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
